{"timestamp": "2025-12-26T23:54:42.634878", "endpoint": "generate-tests", "error": "Could not load model bigscience/bloomz-1b1 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>, <class 'transformers.models.bloom.modeling_bloom.BloomForCausalLM'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 293, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 604, in from_pretrained\n    return model_class.from_pretrained(\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py\", line 277, in _wrapper\n    return func(*args, **kwargs)\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py\", line 4925, in from_pretrained\n    with safe_open(checkpoint_files[0], framework=\"pt\") as f:\nOSError: The paging file is too small for this operation to complete. (os error 1455)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 311, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 604, in from_pretrained\n    return model_class.from_pretrained(\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py\", line 277, in _wrapper\n    return func(*args, **kwargs)\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py\", line 4925, in from_pretrained\n    with safe_open(checkpoint_files[0], framework=\"pt\") as f:\nOSError: The paging file is too small for this operation to complete. (os error 1455)\n\nwhile loading with TFAutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 293, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 607, in from_pretrained\n    raise ValueError(\nValueError: Unrecognized configuration class <class 'transformers.models.bloom.configuration_bloom.BloomConfig'> for this kind of AutoModel: TFAutoModelForCausalLM.\nModel type should be one of BertConfig, CamembertConfig, CTRLConfig, GPT2Config, GPT2Config, GPTJConfig, MistralConfig, OpenAIGPTConfig, OPTConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, TransfoXLConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 311, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 607, in from_pretrained\n    raise ValueError(\nValueError: Unrecognized configuration class <class 'transformers.models.bloom.configuration_bloom.BloomConfig'> for this kind of AutoModel: TFAutoModelForCausalLM.\nModel type should be one of BertConfig, CamembertConfig, CTRLConfig, GPT2Config, GPT2Config, GPTJConfig, MistralConfig, OpenAIGPTConfig, OPTConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, TransfoXLConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig.\n\nwhile loading with BloomForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 293, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py\", line 277, in _wrapper\n    return func(*args, **kwargs)\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py\", line 4925, in from_pretrained\n    with safe_open(checkpoint_files[0], framework=\"pt\") as f:\nOSError: The paging file is too small for this operation to complete. (os error 1455)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 311, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py\", line 277, in _wrapper\n    return func(*args, **kwargs)\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py\", line 4925, in from_pretrained\n    with safe_open(checkpoint_files[0], framework=\"pt\") as f:\nOSError: The paging file is too small for this operation to complete. (os error 1455)\n\n\n", "traceback": "Traceback (most recent call last):\n  File \"C:\\Users\\_Ahmed_\\OneDrive - Zewail City of Science and Technology\\Desktop\\Ai phase3\\AI-assisted-Unit-Test-Generation\\app.py\", line 291, in generate_tests_endpoint\n    tests, prompt, metadata = generate_tests_for_source(request.source_code, cfg)\n  File \"C:\\Users\\_Ahmed_\\OneDrive - Zewail City of Science and Technology\\Desktop\\Ai phase3\\AI-assisted-Unit-Test-Generation\\generate_tests.py\", line 168, in generate_tests_for_source\n    gen_text, raw = call_hf(prompt, cfg, None)\n  File \"C:\\Users\\_Ahmed_\\OneDrive - Zewail City of Science and Technology\\Desktop\\Ai phase3\\AI-assisted-Unit-Test-Generation\\generate_tests.py\", line 122, in call_hf\n    hf_pipe = pipeline(\"text-generation\", model=model_name, device=-1)\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\__init__.py\", line 1027, in pipeline\n    framework, model = infer_framework_load_model(\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 333, in infer_framework_load_model\n    raise ValueError(\nValueError: Could not load model bigscience/bloomz-1b1 with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForCausalLM'>, <class 'transformers.models.bloom.modeling_bloom.BloomForCausalLM'>). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 293, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 604, in from_pretrained\n    return model_class.from_pretrained(\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py\", line 277, in _wrapper\n    return func(*args, **kwargs)\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py\", line 4925, in from_pretrained\n    with safe_open(checkpoint_files[0], framework=\"pt\") as f:\nOSError: The paging file is too small for this operation to complete. (os error 1455)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 311, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 604, in from_pretrained\n    return model_class.from_pretrained(\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py\", line 277, in _wrapper\n    return func(*args, **kwargs)\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py\", line 4925, in from_pretrained\n    with safe_open(checkpoint_files[0], framework=\"pt\") as f:\nOSError: The paging file is too small for this operation to complete. (os error 1455)\n\nwhile loading with TFAutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 293, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 607, in from_pretrained\n    raise ValueError(\nValueError: Unrecognized configuration class <class 'transformers.models.bloom.configuration_bloom.BloomConfig'> for this kind of AutoModel: TFAutoModelForCausalLM.\nModel type should be one of BertConfig, CamembertConfig, CTRLConfig, GPT2Config, GPT2Config, GPTJConfig, MistralConfig, OpenAIGPTConfig, OPTConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, TransfoXLConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 311, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 607, in from_pretrained\n    raise ValueError(\nValueError: Unrecognized configuration class <class 'transformers.models.bloom.configuration_bloom.BloomConfig'> for this kind of AutoModel: TFAutoModelForCausalLM.\nModel type should be one of BertConfig, CamembertConfig, CTRLConfig, GPT2Config, GPT2Config, GPTJConfig, MistralConfig, OpenAIGPTConfig, OPTConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoFormerConfig, TransfoXLConfig, XGLMConfig, XLMConfig, XLMRobertaConfig, XLNetConfig.\n\nwhile loading with BloomForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 293, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py\", line 277, in _wrapper\n    return func(*args, **kwargs)\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py\", line 4925, in from_pretrained\n    with safe_open(checkpoint_files[0], framework=\"pt\") as f:\nOSError: The paging file is too small for this operation to complete. (os error 1455)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 311, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **fp32_kwargs)\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py\", line 277, in _wrapper\n    return func(*args, **kwargs)\n  File \"C:\\Users\\_Ahmed_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py\", line 4925, in from_pretrained\n    with safe_open(checkpoint_files[0], framework=\"pt\") as f:\nOSError: The paging file is too small for this operation to complete. (os error 1455)\n\n\n\n"}
{"timestamp": "2025-12-26T23:59:29.195052", "endpoint": "generate-tests", "provider": "mock", "request": {"source_len": 31, "temperature": 0.0, "max_tokens": 512}, "response": {"tests_len": 145, "syntactic_ok": true}}
{"timestamp": "2025-12-27T00:09:15.796435", "endpoint": "generate-tests-validated", "provider": "mock", "request": {"source_len": 31, "temperature": 0.0, "max_tokens": 512, "run_pytest": true}, "governance": {"safe": true, "syntax_ok": true, "syntax_error": null, "reasons": [], "warnings": [], "pytest_passed": true}}
{"timestamp": "2025-12-27T07:27:36.527746", "endpoint": "generate-tests-validated", "provider": "mock", "request": {"source_len": 31, "temperature": 0.0, "max_tokens": 512, "run_pytest": true}, "governance": {"safe": true, "syntax_ok": true, "syntax_error": null, "reasons": [], "warnings": [], "pytest_passed": true}}
{"timestamp": "2025-12-27T07:34:43.325753", "endpoint": "generate-tests", "provider": "mock", "request": {"source_len": 31, "temperature": 0.0, "max_tokens": 512}, "response": {"tests_len": 145, "syntactic_ok": true}}
{"timestamp": "2025-12-27T07:45:37.611594", "endpoint": "generate-tests", "provider": "mock", "request": {"source_len": 31, "temperature": 0.0, "max_tokens": 512}, "response": {"tests_len": 145, "syntactic_ok": true, "hallucination": true}}
{"timestamp": "2025-12-27T07:46:15.126581", "endpoint": "generate-tests", "provider": "mock", "request": {"source_len": 31, "temperature": 1.0, "max_tokens": 16}, "response": {"tests_len": 145, "syntactic_ok": true, "hallucination": true}}
{"timestamp": "2025-12-27T07:52:18.362416", "endpoint": "generate-tests", "provider": "mock", "request": {"source_len": 31, "temperature": 0.2, "max_tokens": 256}, "response": {"tests_len": 145, "syntactic_ok": true, "hallucination": true}}
{"timestamp": "2025-12-27T07:57:39.540503", "endpoint": "generate-tests", "provider": "mock", "request": {"source_len": 6, "temperature": 1.0, "max_tokens": 16}, "response": {"tests_len": 169, "syntactic_ok": true, "hallucination": true}}
{"timestamp": "2025-12-27T08:34:02.992875", "endpoint": "generate-tests", "provider": "mock", "request": {"source_len": 31, "temperature": 0.2, "max_tokens": 256}, "response": {"tests_len": 145, "syntactic_ok": true, "hallucination": true}}
{"timestamp": "2025-12-27T08:35:43.183970", "endpoint": "generate-tests-validated", "provider": "mock", "request": {"source_len": 31, "temperature": 0.2, "max_tokens": 256, "run_pytest": true}, "governance": {"safe": false, "syntax_ok": true, "syntax_error": null, "reasons": ["Meaningless assertion detected: 'assert True' found in generated tests."], "warnings": [], "pytest_passed": true, "hallucination": true}}
